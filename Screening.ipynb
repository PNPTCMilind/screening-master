{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cb24145",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aldemarogonzalez/anaconda3/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/var/folders/2r/b60vp1dj2zn0rg5yg_gyy0zr0000gq/T/ipykernel_12452/4080736814.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf255868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# organization_path = '../Dataset/bulk_export/organizations.csv' \n",
    "organization_path = '../Dataset/filtered_data/organizations_filtered_by_date_and_roles.csv' \n",
    "organization_data = pd.read_csv(organization_path)\n",
    "\n",
    "# Step 2: Manipulate or analyze the data as needed\n",
    "# For example, you can print the first few rows of the DataFrame using .head()\n",
    "# print(\"First 5 rows of the DataFrame:\")\n",
    "# print(organization_data.head())\n",
    "\n",
    "# Or you can perform some basic analysis, such as getting summary statistics using .describe()\n",
    "# print(\"\\nSummary statistics of the DataFrame:\")\n",
    "# print(organization_data.describe())\n",
    "\n",
    "organization_data_num_rows = organization_data.shape[0]\n",
    "print(\"Number of Organization in the Organization.csv:\", organization_data_num_rows)\n",
    "organization_data_num_columns = organization_data.shape[1] \n",
    "print(\"Number of columns in the Organization.csv:\", organization_data_num_columns)\n",
    "organization_data_column_names = organization_data.columns\n",
    "print(\"Column names in the Organization.csv:\")\n",
    "for name in organization_data_column_names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f861bfd",
   "metadata": {},
   "source": [
    "# Filtering the companies\n",
    "\n",
    "1 - I filtered companies using their creation date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85901b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Define the warm up window \n",
    "start_date = datetime.strptime('2015-01-01', '%Y-%m-%d') # Start Day: 1 of January of 2015\n",
    "end_date = datetime.strptime('2018-12-31', '%Y-%m-%d') # End Day: 31 of December of 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30d2c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('../Dataset/bulk_export/organizations.csv', 'r') as f, open('../Dataset/filtered_data/organizations_filtered_by_date.csv', 'w', newline='') as f_out:\n",
    "    reader = csv.reader(f)\n",
    "    writer = csv.writer(f_out)\n",
    "\n",
    "    header = next(reader)\n",
    "    writer.writerow(header)\n",
    "\n",
    "    date_index = header.index('founded_on')\n",
    "\n",
    "    # Read the rest of the csv and write rows where the date is within the range\n",
    "    for row in reader:\n",
    "        date_str = row[date_index]\n",
    "        if date_str:  # Check if the date string is not empty\n",
    "            try:\n",
    "                date = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "                if start_date <= date <= end_date:\n",
    "                    writer.writerow(row)\n",
    "            except ValueError:\n",
    "                print(\"Error: Unable to parse date string '{}'\".format(date_str))\n",
    "        else:\n",
    "            print(\"Warning: Empty date string encountered in row {}\".format(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14e29d8",
   "metadata": {},
   "source": [
    "Looking at the dataset, we see that there is a column that also let us differentiate between an Companies and Investment firms in the domain column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbe5da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Dataset/filtered_data/organizations_filtered_by_date.csv', 'r') as f, open('../Dataset/filtered_data/organizations_filtered_by_date_and_roles.csv', 'w', newline='') as f_out:\n",
    "    reader = csv.reader(f)\n",
    "    writer = csv.writer(f_out)\n",
    "\n",
    "    header = next(reader)  # This gets the header row\n",
    "    writer.writerow(header)  # Write the header to the new file\n",
    "\n",
    "    # Find the index of the date column (replace 'date_column' with your column's name)\n",
    "    roles_index = header.index('roles')\n",
    "\n",
    "    # Read the rest of the csv and write rows where the date is within the range\n",
    "    for row in reader:\n",
    "        roles_string = row[roles_index]\n",
    "        if roles_string:  # Check if the date string is not empty\n",
    "            try:\n",
    "                if roles_string == \"company\":\n",
    "                    writer.writerow(row)\n",
    "            except ValueError:\n",
    "                print(\"Error: Unable to parse string string '{}'\".format(roles_string))\n",
    "        else:\n",
    "            print(\"Warning: Empty string string encountered in row {}\".format(row))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd53bf4",
   "metadata": {},
   "source": [
    "Clean the database by dropping the columns that are not relevant for the study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa14662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the columns to drop\n",
    "columns_to_drop = ['type', \n",
    "    'cb_url', \n",
    "    'rank', \n",
    "    'created_at', \n",
    "    'updated_at', \n",
    "    'legal_name', \n",
    "    'roles', \n",
    "    'cb_url', \n",
    "    'rank', \n",
    "    'created_at', \n",
    "    'updated_at', \n",
    "    'legal_name', \n",
    "    'roles', \n",
    "    'email', \n",
    "    'phone', \n",
    "    'logo_url', \n",
    "    'alias1', \n",
    "    'alias2', \n",
    "    'alias3', \n",
    "    'primary_role', \n",
    "    'num_exits']\n",
    "\n",
    "# Drop the specified columns\n",
    "organizations_data = organization_data.drop(columns=columns_to_drop)\n",
    "organizations_data.to_csv('../Dataset/filtered_data/filtered_organization.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59944f9e",
   "metadata": {},
   "source": [
    "### Now I merge the organizations with their funding data, IPO data, closure data to further filtering by IPO/Acquisition/Closure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48fb0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "funding_path = '../Dataset/bulk_export/funding_rounds.csv' \n",
    "\n",
    "funding_data = pd.read_csv(funding_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05904bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "organizations_path = '../Dataset/filtered_data/filtered_organization.csv'\n",
    "\n",
    "organizations_data = pd.read_csv(organizations_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346511c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = organizations_data.merge(funding_data, left_on='uuid', right_on='org_uuid', how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750c9346",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('../Dataset/filtered_data/filtered_organization_merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1806035a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_path = '../Dataset/filtered_data/filtered_organization_merged.csv' \n",
    "\n",
    "merged_df = pd.read_csv(merged_df_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990c1540",
   "metadata": {},
   "source": [
    "First - I removed the ones closed during the Warmup window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c0d5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['closed_on'] = pd.to_datetime(merged_df['closed_on'])\n",
    "filtered_close_merged_df = merged_df[(merged_df['closed_on'] < start_date) | (merged_df['closed_on'] > end_date) | (merged_df['closed_on'].isna())]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ec9fb2",
   "metadata": {},
   "source": [
    "Now I removed the ones acquired during the Warmup window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42754ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "acquisitions_path = '../Dataset/bulk_export/acquisitions.csv'  # Replace 'your_file.csv' with the actual path to your CSV file\n",
    "acquisitions_data = pd.read_csv(acquisitions_path)\n",
    "acquisitions_data_num_rows = acquisitions_data.shape[0] \n",
    "print(\"Number of acquisitions in the acquisitions.csv:\", acquisitions_data_num_rows)\n",
    "acquisitions_data_num_columns = acquisitions_data.shape[1] \n",
    "print(\"Number of columns in the acquisitions.csv:\", acquisitions_data_num_columns)\n",
    "acquisitions_data_column_names = acquisitions_data.columns\n",
    "print(\"Column names in the acquisitions.csv:\")\n",
    "for name in acquisitions_data_column_names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c32caac",
   "metadata": {},
   "outputs": [],
   "source": [
    "acquired_df =  filtered_close_merged_df.merge(acquisitions_data, left_on='uuid_x', right_on='acquiree_uuid', how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bf041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "acquired_df.to_csv('../Dataset/filtered_data/acquired_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4215ca08",
   "metadata": {},
   "outputs": [],
   "source": [
    "acquired_df['acquired_on'] = pd.to_datetime(acquired_df['acquired_on'])\n",
    "filtered_close_acquired_df = acquired_df[(acquired_df['acquired_on'] < start_date) | (acquired_df['acquired_on'] > end_date) | (acquired_df['acquired_on'].isna())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8acf60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_close_acquired_df.to_csv('../Dataset/filtered_data/filtered_close_acquired_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6521219e",
   "metadata": {},
   "source": [
    "Now I removed the ones that IPO during the Warmup window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ad0a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipos_path = '../Dataset/bulk_export/ipos.csv'  # Replace 'your_file.csv' with the actual path to your CSV file\n",
    "ipos_data = pd.read_csv(ipos_path)\n",
    "ipos_data_num_rows = ipos_data.shape[0] \n",
    "print(\"Number of ipos in the ipos.csv:\", ipos_data_num_rows)\n",
    "ipos_data_num_columns = ipos_data.shape[1] \n",
    "print(\"Number of columns in the ipos.csv:\", ipos_data_num_columns)\n",
    "ipos_data_column_names = ipos_data.columns\n",
    "print(\"Column names in the ipos.csv:\")\n",
    "for name in ipos_data_column_names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999ec00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns before merging to ensure they have unique names\n",
    "filtered_close_acquired_df.rename(columns={'uuid_x': 'uuid_org'}, inplace=True)\n",
    "filtered_close_acquired_df.rename(columns={'name_x': 'name_org'}, inplace=True)\n",
    "filtered_close_acquired_df.rename(columns={'permalink_x': 'permalink_org'}, inplace=True)\n",
    "\n",
    "ipos_data.rename(columns={'org_uuid': 'uuid_ipos'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00920a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipos_df =  filtered_close_acquired_df.merge(ipos_data, left_on='uuid_org', right_on='uuid_ipos', how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a225d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipos_df.to_csv('../Dataset/filtered_data/ipos_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd370692",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipos_df['went_public_on'] = pd.to_datetime(ipos_df['went_public_on'])\n",
    "filtered_close_acquired_ipos_df = ipos_df[(ipos_df['went_public_on'] < start_date) | (ipos_df['went_public_on'] > end_date) | (ipos_df['went_public_on'].isna())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d2d0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_close_acquired_ipos_df.to_csv('../Dataset/filtered_data/filtered_close_acquired_ipos_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bafcc26",
   "metadata": {},
   "source": [
    "## I filter the companies which closed a funding round above Series B or above, which is not interesting for Plug and Play investments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a10a709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the series that are Series B or above\n",
    "series_b_or_above = ['series_b', 'series_c', 'series_d', 'series_e', 'series_f', 'series_g']\n",
    "\n",
    "# Filter the DataFrame to exclude organizations that have a series B or above in their investment_type\n",
    "\n",
    "organizations_to_remove = filtered_close_acquired_ipos_df[filtered_close_acquired_ipos_df['investment_type'].isin(series_b_or_above)]['org_uuid'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6013df",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_series_df = filtered_close_acquired_ipos_df[~filtered_close_acquired_ipos_df['org_uuid'].isin(organizations_to_remove)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c35dc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_series_df.to_csv('../Dataset/filtered_data/filtered_series_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5eaed7",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8521b142",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_path = '../Dataset/filtered_data/filtered_series_df.csv'\n",
    "\n",
    "filtered_data = pd.read_csv(filtered_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8537d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some companies are duplicate because they had a few rounds, this is for taking them away\n",
    "unique_filtered = filtered_data.drop_duplicates(subset=['uuid_org'], keep='first')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c5610f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = unique_filtered.shape[0]\n",
    "\n",
    "print(f\"There is {num_rows} companies.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4fa86e",
   "metadata": {},
   "source": [
    "Now we need to clean the data set by removing unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbf357e",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = unique_filtered.columns.tolist()\n",
    "\n",
    "# Join the column names with commas\n",
    "column_names_str = '\\', \\''.join(column_names)\n",
    "\n",
    "# Print the column names\n",
    "print(column_names_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11290f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['uuid_y', 'name_y', 'type_x', 'permalink_y', 'cb_url_x', 'rank_x', 'created_at_x', 'updated_at_x', 'country_code_y', 'state_code_y', 'region_y', 'city_y', 'investment_type', 'announced_on', 'raised_amount_usd', 'raised_amount', 'raised_amount_currency_code', 'post_money_valuation_usd', 'post_money_valuation', 'post_money_valuation_currency_code', 'investor_count', 'org_uuid', 'org_name_x', 'lead_investor_uuids', 'uuid_x', 'name_x', 'type_y', 'permalink_x', 'cb_url_y', 'rank_y', 'created_at_y', 'updated_at_y', 'acquiree_uuid', 'acquiree_name', 'acquiree_cb_url', 'acquiree_country_code', 'acquiree_state_code', 'acquiree_region', 'acquiree_city', 'acquirer_uuid', 'acquirer_name', 'acquirer_cb_url', 'acquirer_country_code', 'acquirer_state_code', 'acquirer_region', 'acquirer_city', 'acquisition_type', 'acquired_on', 'price_usd', 'price', 'price_currency_code', 'uuid_y.1', 'name_y.1', 'type', 'permalink_y.1', 'cb_url', 'rank', 'created_at', 'updated_at', 'uuid_ipos', 'org_name_y', 'org_cb_url', 'country_code', 'state_code', 'region', 'city', 'stock_exchange_symbol', 'stock_symbol', 'went_public_on', 'share_price_usd', 'share_price', 'share_price_currency_code', 'valuation_price_usd', 'valuation_price', 'valuation_price_currency_code', 'money_raised_usd', 'money_raised', 'money_raised_currency_code']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d77e7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_filtered = unique_filtered.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20c1d5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uuid_org', 'name_org', 'permalink_org', 'domain', 'homepage_url', 'country_code_x', 'state_code_x', 'region_x', 'city_x', 'address', 'postal_code', 'status', 'short_description', 'category_list', 'category_groups_list', 'num_funding_rounds', 'total_funding_usd', 'total_funding', 'total_funding_currency_code', 'founded_on', 'last_funding_on', 'closed_on', 'employee_count', 'facebook_url', 'linkedin_url', 'twitter_url\n"
     ]
    }
   ],
   "source": [
    "column_names = unique_filtered.columns.tolist()\n",
    "\n",
    "# Join the column names with commas\n",
    "column_names_str = '\\', \\''.join(column_names)\n",
    "\n",
    "# Print the column names\n",
    "print(column_names_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facda5bd",
   "metadata": {},
   "source": [
    "## Now we need to add the predictor variables \n",
    "\n",
    "And also delete some attributes that I do not consider relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc885940",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_filtered.rename(columns={'country_code_x': 'country_code'}, inplace=True)\n",
    "unique_filtered.rename(columns={'state_code_x': 'state_code'}, inplace=True)\n",
    "unique_filtered.rename(columns={'region_x': 'region'}, inplace=True)\n",
    "unique_filtered.rename(columns={'city_x': 'city'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6aef5887",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_filtered.to_csv('../Dataset/filtered_data/unique_filtered.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "400546d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_filtered = unique_filtered.drop(columns='employee_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf63205",
   "metadata": {},
   "source": [
    "Nota para alde: creo que deberia quitar las compañias que tengan al menos 1 funding round pero no diga la cantidad... No lo se\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc767f38",
   "metadata": {},
   "source": [
    "## Company Information\n",
    "\n",
    "we included the company age in months (age_months) at the beginning of the simulation (ts) and we added variables that measure the presence of the company in social media networks. \n",
    "\n",
    "These binary variables indicate whether the company registered in Crunchbase its con- tact information (has_email and has_phone) or a Facebook (has_facebook_url), Twitter (has_twitter_url) or Linkedin (has_linkedin_url) account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "089a48c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Beginning of the simulation period\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the warm up window \n",
    "simulation_start_date = datetime.strptime('2019-01-01', '%Y-%m-%d') # Start Day: 1 of January of 2019\n",
    "simulation_end_date = datetime.strptime('2022-12-31', '%Y-%m-%d') # End Day: 31 of December of 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c1e2cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "unique_filtered['founded_on'] = pd.to_datetime(unique_filtered['founded_on'])\n",
    "\n",
    "unique_filtered['age_months'] = ((simulation_start_date - unique_filtered['founded_on']).dt.days / 30).apply(math.ceil)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f7d999a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to check if the URL is present\n",
    "def has_url(url):\n",
    "    return 1 if pd.notnull(url) else 0\n",
    "\n",
    "# Convert URLs into binary variables\n",
    "unique_filtered['has_facebook_url'] = unique_filtered['facebook_url'].apply(has_url)\n",
    "unique_filtered['has_twitter_url'] = unique_filtered['twitter_url'].apply(has_url)\n",
    "unique_filtered['has_linkedin_url'] = unique_filtered['linkedin_url'].apply(has_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08c5e37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_filtered.to_csv('../Dataset/filtered_data/unique_filtered.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c13e1d4",
   "metadata": {},
   "source": [
    "## Funding information\n",
    "\n",
    "The variables in this category summarize the funding events occurred in a company during the Warmup window. The availability of a temporal series of funding round events in Crunchbase allowed us to synthesize information about:\n",
    "\n",
    "- Number of funding rounds that the company achieved before ts (round_count) and the total amount raised in those rounds(raised_amount_usd)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcce9274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of funding_rounds in the funding_rounds.csv: 631092\n",
      "Number of columns in the funding_rounds.csv: 24\n",
      "Column names in the funding_rounds.csv:\n",
      "uuid\n",
      "name\n",
      "type\n",
      "permalink\n",
      "cb_url\n",
      "rank\n",
      "created_at\n",
      "updated_at\n",
      "country_code\n",
      "state_code\n",
      "region\n",
      "city\n",
      "investment_type\n",
      "announced_on\n",
      "raised_amount_usd\n",
      "raised_amount\n",
      "raised_amount_currency_code\n",
      "post_money_valuation_usd\n",
      "post_money_valuation\n",
      "post_money_valuation_currency_code\n",
      "investor_count\n",
      "org_uuid\n",
      "org_name\n",
      "lead_investor_uuids\n"
     ]
    }
   ],
   "source": [
    "funding_rounds_path = '../Dataset/bulk_export/funding_rounds.csv' \n",
    "funding_rounds_data = pd.read_csv(funding_rounds_path)\n",
    "\n",
    "# Step 2: Manipulate or analyze the data as needed\n",
    "# For example, you can print the first few rows of the DataFrame using .head()\n",
    "# print(\"First 5 rows of the DataFrame:\")\n",
    "# print(funding_rounds_data.head())\n",
    "\n",
    "# Or you can perform some basic analysis, such as getting summary statistics using .describe()\n",
    "# print(\"\\nSummary statistics of the DataFrame:\")\n",
    "# print(funding_rounds_data.describe())\n",
    "\n",
    "funding_rounds_data_num_rows = funding_rounds_data.shape[0]\n",
    "print(\"Number of funding_rounds in the funding_rounds.csv:\", funding_rounds_data_num_rows)\n",
    "funding_rounds_data_num_columns = funding_rounds_data.shape[1] \n",
    "print(\"Number of columns in the funding_rounds.csv:\", funding_rounds_data_num_columns)\n",
    "funding_rounds_data_column_names = funding_rounds_data.columns\n",
    "print(\"Column names in the funding_rounds.csv:\")\n",
    "for name in funding_rounds_data_column_names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "675d3446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 449051 companies.\n"
     ]
    }
   ],
   "source": [
    "unique_filtered = pd.read_csv('../Dataset/filtered_data/unique_filtered.csv')\n",
    "\n",
    "merged_unique_funding_df = unique_filtered.merge(funding_rounds_data, left_on='uuid_org', right_on='org_uuid', how=\"left\")\n",
    "num_rows = merged_unique_funding_df.shape[0]\n",
    "\n",
    "print(f\"There is {num_rows} companies.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d911652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in the funding_rounds.csv:\n",
      "uuid_org\n",
      "name_org\n",
      "permalink_org\n",
      "domain\n",
      "homepage_url\n",
      "country_code_x\n",
      "state_code_x\n",
      "region_x\n",
      "city_x\n",
      "address\n",
      "postal_code\n",
      "status\n",
      "short_description\n",
      "category_list\n",
      "category_groups_list\n",
      "num_funding_rounds\n",
      "total_funding_usd\n",
      "total_funding\n",
      "total_funding_currency_code\n",
      "founded_on\n",
      "last_funding_on\n",
      "closed_on\n",
      "facebook_url\n",
      "linkedin_url\n",
      "twitter_url\n",
      "age_months\n",
      "has_facebook_url\n",
      "has_twitter_url\n",
      "has_linkedin_url\n",
      "uuid\n",
      "name\n",
      "type\n",
      "permalink\n",
      "cb_url\n",
      "rank\n",
      "created_at\n",
      "updated_at\n",
      "country_code_y\n",
      "state_code_y\n",
      "region_y\n",
      "city_y\n",
      "investment_type\n",
      "announced_on\n",
      "raised_amount_usd\n",
      "raised_amount\n",
      "raised_amount_currency_code\n",
      "post_money_valuation_usd\n",
      "post_money_valuation\n",
      "post_money_valuation_currency_code\n",
      "investor_count\n",
      "org_uuid\n",
      "org_name\n",
      "lead_investor_uuids\n"
     ]
    }
   ],
   "source": [
    "merged_unique_funding_df_column_names = merged_unique_funding_df.columns\n",
    "print(\"Column names in the funding_rounds.csv:\")\n",
    "for name in merged_unique_funding_df_column_names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f313149",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2r/b60vp1dj2zn0rg5yg_gyy0zr0000gq/T/ipykernel_12452/1756900613.py:18: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0' has dtype incompatible with datetime64[ns], please explicitly cast to a compatible dtype first.\n",
      "  merged_unique_funding_df.fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Convert 'announced_on' column to datetime if it's not already\n",
    "merged_unique_funding_df['announced_on'] = pd.to_datetime(merged_unique_funding_df['announced_on'])\n",
    "\n",
    "# Filter the funding rounds that occurred before ts\n",
    "funding_before_ts = merged_unique_funding_df[merged_unique_funding_df['announced_on'] < simulation_start_date]\n",
    "\n",
    "# Count the number of funding rounds for each company\n",
    "round_count = funding_before_ts.groupby('uuid_org').size().rename('round_count')\n",
    "\n",
    "# Sum up the raised amount in those rounds for each company\n",
    "raised_amount_usd = funding_before_ts.groupby('uuid_org')['raised_amount_usd'].sum().rename('raised_amount_usd')\n",
    "\n",
    "# Merge round count and raised amount into merged_unique_funding_df DataFrame\n",
    "merged_unique_funding_df = pd.merge(merged_unique_funding_df, round_count, left_on='uuid_org', right_index=True, how='left')\n",
    "merged_unique_funding_df = pd.merge(merged_unique_funding_df, raised_amount_usd, left_on='uuid_org', right_index=True, how='left')\n",
    "\n",
    "# Replace NaN values with 0 for companies with no data\n",
    "merged_unique_funding_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b78dd7",
   "metadata": {},
   "source": [
    "- Data about the last funding round in Warmup window: funding round type (last_round_investment_type), amount raised(last_round_raised_amount_usd), company valuation after this funding round(last_round_post_money_valuation) and time lapsed, in months, between the beginning of the simulation (ts) and when this funding round occurred(last_round_timelapse_months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e9a29b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2r/b60vp1dj2zn0rg5yg_gyy0zr0000gq/T/ipykernel_12452/3126441978.py:21: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  merged_unique_funding_df.fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# 2. Find the last funding round for each company within the Warmup window\n",
    "last_round_warmup = funding_before_ts.groupby('uuid_org').last()\n",
    "\n",
    "# 3. Extract the required data from the last funding round for each company\n",
    "last_round_investment_type = last_round_warmup['investment_type']\n",
    "last_round_raised_amount_usd = last_round_warmup['raised_amount_usd']\n",
    "last_round_post_money_valuation = last_round_warmup['post_money_valuation_usd']\n",
    "\n",
    "# 4. Calculate the time lapse in months between simulation start date and the last funding round\n",
    "last_round_timelapse_months = ((simulation_start_date - last_round_warmup['announced_on']).dt.days / 30).apply(math.ceil).astype(int)\n",
    "\n",
    "# Add extracted data to merged_unique_funding_df DataFrame\n",
    "merged_unique_funding_df['last_round_investment_type'] = last_round_investment_type\n",
    "merged_unique_funding_df['last_round_raised_amount_usd'] = last_round_raised_amount_usd\n",
    "merged_unique_funding_df['last_round_post_money_valuation'] = last_round_post_money_valuation\n",
    "merged_unique_funding_df['last_round_timelapse_months'] = last_round_timelapse_months\n",
    "\n",
    "# Replace NaN values with 0 for companies with no data\n",
    "merged_unique_funding_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd49182",
   "metadata": {},
   "source": [
    "- Number of (unique) investors who participated in the funding rounds during the Warmup window (investor_count) and specifically, in the last funding (last_round_investor_count)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef3c56cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of investments in the investments.csv: 1027342\n",
      "Number of columns in the investments.csv: 14\n",
      "Column names in the investments.csv:\n",
      "uuid\n",
      "name\n",
      "type\n",
      "permalink\n",
      "cb_url\n",
      "rank\n",
      "created_at\n",
      "updated_at\n",
      "funding_round_uuid\n",
      "funding_round_name\n",
      "investor_uuid\n",
      "investor_name\n",
      "investor_type\n",
      "is_lead_investor\n"
     ]
    }
   ],
   "source": [
    "investments_path = '../Dataset/bulk_export/investments.csv' \n",
    "investments_data = pd.read_csv(investments_path)\n",
    "\n",
    "# Step 2: Manipulate or analyze the data as needed\n",
    "# For example, you can print the first few rows of the DataFrame using .head()\n",
    "# print(\"First 5 rows of the DataFrame:\")\n",
    "# print(investments_data.head())\n",
    "\n",
    "# Or you can perform some basic analysis, such as getting summary statistics using .describe()\n",
    "# print(\"\\nSummary statistics of the DataFrame:\")\n",
    "# print(investments_data.describe())\n",
    "\n",
    "investments_data_num_rows = investments_data.shape[0]\n",
    "print(\"Number of investments in the investments.csv:\", investments_data_num_rows)\n",
    "investments_data_num_columns = investments_data.shape[1] \n",
    "print(\"Number of columns in the investments.csv:\", investments_data_num_columns)\n",
    "investments_data_column_names = investments_data.columns\n",
    "print(\"Column names in the investments.csv:\")\n",
    "for name in investments_data_column_names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "624ef435",
   "metadata": {},
   "outputs": [],
   "source": [
    "investments_filtered =  merged_unique_funding_df.merge(investments_data, right_on='funding_round_uuid', left_on='uuid', how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "731276b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of investors in the investors.csv: 274366\n",
      "Number of columns in the investors.csv: 25\n",
      "Column names in the investors.csv:\n",
      "uuid\n",
      "name\n",
      "type\n",
      "permalink\n",
      "cb_url\n",
      "rank\n",
      "created_at\n",
      "updated_at\n",
      "roles\n",
      "domain\n",
      "country_code\n",
      "state_code\n",
      "region\n",
      "city\n",
      "investor_types\n",
      "investment_count\n",
      "total_funding_usd\n",
      "total_funding\n",
      "total_funding_currency_code\n",
      "founded_on\n",
      "closed_on\n",
      "facebook_url\n",
      "linkedin_url\n",
      "twitter_url\n",
      "logo_url\n"
     ]
    }
   ],
   "source": [
    "investors_path = '../Dataset/bulk_export/investors.csv' \n",
    "investors_data = pd.read_csv(investors_path)\n",
    "\n",
    "# Step 2: Manipulate or analyze the data as needed\n",
    "# For example, you can print the first few rows of the DataFrame using .head()\n",
    "# print(\"First 5 rows of the DataFrame:\")\n",
    "# print(investors_data.head())\n",
    "\n",
    "# Or you can perform some basic analysis, such as getting summary statistics using .describe()\n",
    "# print(\"\\nSummary statistics of the DataFrame:\")\n",
    "# print(investors_data.describe())\n",
    "\n",
    "investors_data_num_rows = investors_data.shape[0]\n",
    "print(\"Number of investors in the investors.csv:\", investors_data_num_rows)\n",
    "investors_data_num_columns = investors_data.shape[1] \n",
    "print(\"Number of columns in the investors.csv:\", investors_data_num_columns)\n",
    "investors_data_column_names = investors_data.columns\n",
    "print(\"Column names in the investors.csv:\")\n",
    "for name in investors_data_column_names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36589dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "investments_filtered_investors =  investments_filtered.merge(investors_data, right_on='uuid', left_on='investor_uuid', how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193a74ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d55e52ff",
   "metadata": {},
   "source": [
    "- Qualitative information about the investors. We defined a category of renowned investors as the investment companies registered in Crunchbase and created variables for the number of unique renowned investors who participated in the funding rounds during the Warmup window (known_investor_count) and, specifically, in the last funding (last_round_known_investor_count)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c77682a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21decd3a",
   "metadata": {},
   "source": [
    "## Founders information\n",
    "The last category comprises information about the people who founded a company. In addition to the number of founder(founders_count), we synthesized new variables that provide information about the heterogeneity of the founders according to their origin number of different countries where the founders come from(founders_dif_country_count)– and gender –number of male\n",
    "(founders_male_count) and female (founders_female_count)founders.\n",
    "\n",
    "Previous studies highlighted the importance of having a college education when building a new company. Crunchbase provides information about the education of most of the founders. However, the way this information is stored (in a free-form text) hinders the synthesis of qualitative variables about the education received by company founders.\n",
    "\n",
    "After revisiting the information contained in Crunchbase and observing that most of the education entries refer to higher education, we decided to synthesize quantitative variables about the total number of degrees obtained by company founders (founders_degree_count_total), as well as the maximum (founders_degree_count_max) and the average number of degrees (founders_degree_count_mean) among them.\n",
    "\n",
    "The sparsity problem is evident also in this category, as most of the companies do not have information about their founders or their education. In this case, we consider the absence of data as useful information and consider 0, where it corresponds. It means that the company has not updated the information about the founders in Crunchbase"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
