{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cb24145",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aldemarogonzalez/anaconda3/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/var/folders/2r/b60vp1dj2zn0rg5yg_gyy0zr0000gq/T/ipykernel_3435/4080736814.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf255868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# organization_path = '../Dataset/bulk_export/organizations.csv' \n",
    "organization_path = '../Dataset/filtered_data/organizations_filtered_by_date_and_roles.csv' \n",
    "organization_data = pd.read_csv(organization_path)\n",
    "\n",
    "# Step 2: Manipulate or analyze the data as needed\n",
    "# For example, you can print the first few rows of the DataFrame using .head()\n",
    "# print(\"First 5 rows of the DataFrame:\")\n",
    "# print(organization_data.head())\n",
    "\n",
    "# Or you can perform some basic analysis, such as getting summary statistics using .describe()\n",
    "# print(\"\\nSummary statistics of the DataFrame:\")\n",
    "# print(organization_data.describe())\n",
    "\n",
    "organization_data_num_rows = organization_data.shape[0]\n",
    "print(\"Number of Organization in the Organization.csv:\", organization_data_num_rows)\n",
    "organization_data_num_columns = organization_data.shape[1] \n",
    "print(\"Number of columns in the Organization.csv:\", organization_data_num_columns)\n",
    "organization_data_column_names = organization_data.columns\n",
    "print(\"Column names in the Organization.csv:\")\n",
    "for name in organization_data_column_names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cdaac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_path = '../Dataset/bulk_export/people.csv'  # Replace 'your_file.csv' with the actual path to your CSV file\n",
    "people_data = pd.read_csv(people_path)\n",
    "people_data_num_rows = people_data.shape[0] \n",
    "print(\"Number of People in the People.csv:\", people_data_num_rows)\n",
    "people_data_num_columns = people_data.shape[1] \n",
    "print(\"Number of columns in the People.csv:\", people_data_num_columns)\n",
    "people_data_column_names = people_data.columns\n",
    "print(\"Column names in the People.csv:\")\n",
    "for name in people_data_column_names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac4dc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "funding_path = '../Dataset/bulk_export/funding_rounds.csv'\n",
    "funding_data = pd.read_csv(funding_path)\n",
    "funding_data_num_rows = funding_data.shape[0] \n",
    "print(\"Number of funding in the funding.csv:\", funding_data_num_rows)\n",
    "funding_data_num_columns = funding_data.shape[1]\n",
    "print(\"Number of columns in the funding.csv:\", funding_data_num_columns)\n",
    "funding_data_column_names = funding_data.columns\n",
    "print(\"Column names in the funding.csv:\")\n",
    "for name in funding_data_column_names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f861bfd",
   "metadata": {},
   "source": [
    "# Filtering the companies\n",
    "\n",
    "1 - I filtered companies using their creation date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b85901b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Define the warm up window \n",
    "start_date = datetime.strptime('2015-01-01', '%Y-%m-%d') # Start Day: 1 of January of 2015\n",
    "end_date = datetime.strptime('2018-12-31', '%Y-%m-%d') # End Day: 31 of December of 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30d2c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('../Dataset/bulk_export/organizations.csv', 'r') as f, open('../Dataset/filtered_data/organizations_filtered_by_date.csv', 'w', newline='') as f_out:\n",
    "    reader = csv.reader(f)\n",
    "    writer = csv.writer(f_out)\n",
    "\n",
    "    header = next(reader)\n",
    "    writer.writerow(header)\n",
    "\n",
    "    date_index = header.index('founded_on')\n",
    "\n",
    "    # Read the rest of the csv and write rows where the date is within the range\n",
    "    for row in reader:\n",
    "        date_str = row[date_index]\n",
    "        if date_str:  # Check if the date string is not empty\n",
    "            try:\n",
    "                date = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "                if start_date <= date <= end_date:\n",
    "                    writer.writerow(row)\n",
    "            except ValueError:\n",
    "                print(\"Error: Unable to parse date string '{}'\".format(date_str))\n",
    "        else:\n",
    "            print(\"Warning: Empty date string encountered in row {}\".format(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14e29d8",
   "metadata": {},
   "source": [
    "Looking at the dataset, we see that there is a column that also let us differentiate between an Companies and Investment firms in the domain column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbe5da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Dataset/filtered_data/organizations_filtered_by_date.csv', 'r') as f, open('../Dataset/filtered_data/organizations_filtered_by_date_and_roles.csv', 'w', newline='') as f_out:\n",
    "    reader = csv.reader(f)\n",
    "    writer = csv.writer(f_out)\n",
    "\n",
    "    header = next(reader)  # This gets the header row\n",
    "    writer.writerow(header)  # Write the header to the new file\n",
    "\n",
    "    # Find the index of the date column (replace 'date_column' with your column's name)\n",
    "    roles_index = header.index('roles')\n",
    "\n",
    "    # Read the rest of the csv and write rows where the date is within the range\n",
    "    for row in reader:\n",
    "        roles_string = row[roles_index]\n",
    "        if roles_string:  # Check if the date string is not empty\n",
    "            try:\n",
    "                if roles_string == \"company\":\n",
    "                    writer.writerow(row)\n",
    "            except ValueError:\n",
    "                print(\"Error: Unable to parse string string '{}'\".format(roles_string))\n",
    "        else:\n",
    "            print(\"Warning: Empty string string encountered in row {}\".format(row))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd53bf4",
   "metadata": {},
   "source": [
    "Clean the database by dropping the columns that are not relevant for the study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa14662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the columns to drop\n",
    "columns_to_drop = ['type', \n",
    "    'cb_url', \n",
    "    'rank', \n",
    "    'created_at', \n",
    "    'updated_at', \n",
    "    'legal_name', \n",
    "    'roles', \n",
    "    'cb_url', \n",
    "    'rank', \n",
    "    'created_at', \n",
    "    'updated_at', \n",
    "    'legal_name', \n",
    "    'roles', \n",
    "    'email', \n",
    "    'phone', \n",
    "    'logo_url', \n",
    "    'alias1', \n",
    "    'alias2', \n",
    "    'alias3', \n",
    "    'primary_role', \n",
    "    'num_exits']\n",
    "\n",
    "# Drop the specified columns\n",
    "organizations_data = organization_data.drop(columns=columns_to_drop)\n",
    "organizations_data.to_csv('../Dataset/filtered_data/filtered_organization.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59944f9e",
   "metadata": {},
   "source": [
    "### Now I merge the organizations with their funding data to further filtering by Venture Stage/Acquisition/Closure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b48fb0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "funding_path = '../Dataset/bulk_export/funding_rounds.csv' \n",
    "\n",
    "funding_data = pd.read_csv(funding_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05904bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "organizations_path = '../Dataset/filtered_data/filtered_organization.csv'\n",
    "\n",
    "organizations_data = pd.read_csv(organizations_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "346511c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = organizations_data.merge(funding_data, left_on='uuid', right_on='org_uuid', how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "750c9346",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('../Dataset/filtered_data/filtered_organization_merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990c1540",
   "metadata": {},
   "source": [
    "First - I removed the ones closed during the Warmup window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8c0d5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['closed_on'] = pd.to_datetime(merged_df['closed_on'])\n",
    "filtered_close_merged_df = merged_df[(merged_df['closed_on'] < start_date) | (merged_df['closed_on'] > end_date) | (merged_df['closed_on'].isna())]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ec9fb2",
   "metadata": {},
   "source": [
    "Now I removed the ones acquired during the Warmup window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42754ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of acquisitions in the acquisitions.csv: 162216\n",
      "Number of columns in the acquisitions.csv: 27\n",
      "Column names in the acquisitions.csv:\n",
      "uuid\n",
      "name\n",
      "type\n",
      "permalink\n",
      "cb_url\n",
      "rank\n",
      "created_at\n",
      "updated_at\n",
      "acquiree_uuid\n",
      "acquiree_name\n",
      "acquiree_cb_url\n",
      "acquiree_country_code\n",
      "acquiree_state_code\n",
      "acquiree_region\n",
      "acquiree_city\n",
      "acquirer_uuid\n",
      "acquirer_name\n",
      "acquirer_cb_url\n",
      "acquirer_country_code\n",
      "acquirer_state_code\n",
      "acquirer_region\n",
      "acquirer_city\n",
      "acquisition_type\n",
      "acquired_on\n",
      "price_usd\n",
      "price\n",
      "price_currency_code\n"
     ]
    }
   ],
   "source": [
    "acquisitions_path = '../Dataset/bulk_export/acquisitions.csv'  # Replace 'your_file.csv' with the actual path to your CSV file\n",
    "acquisitions_data = pd.read_csv(acquisitions_path)\n",
    "acquisitions_data_num_rows = acquisitions_data.shape[0] \n",
    "print(\"Number of acquisitions in the acquisitions.csv:\", acquisitions_data_num_rows)\n",
    "acquisitions_data_num_columns = acquisitions_data.shape[1] \n",
    "print(\"Number of columns in the acquisitions.csv:\", acquisitions_data_num_columns)\n",
    "acquisitions_data_column_names = acquisitions_data.columns\n",
    "print(\"Column names in the acquisitions.csv:\")\n",
    "for name in acquisitions_data_column_names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c32caac",
   "metadata": {},
   "outputs": [],
   "source": [
    "acquired_df =  filtered_close_merged_df.merge(acquisitions_data, left_on='uuid_x', right_on='acquiree_uuid', how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81bf041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "acquired_df.to_csv('../Dataset/filtered_data/acquired_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4215ca08",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['acquired_on'] = pd.to_datetime(merged_df['acquired_on'])\n",
    "filtered_close_merged_df = merged_df[(merged_df['acquired_on'] < start_date) | (merged_df['acquired_on'] > end_date) | (merged_df['acquired_on'].isna())]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6521219e",
   "metadata": {},
   "source": [
    "Now I removed the ones that IPO during the Warmup window "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bafcc26",
   "metadata": {},
   "source": [
    "Second - I filter the companies which closed a funding round above Series B or above, which is not interesting for Plug and Play investments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7ca03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Dataset/filtered_data/organizations_filtered_by_date.csv', 'r') as f, open('../Dataset/filtered_data/organizations_filtered_by_funding.csv', 'w', newline='') as f_out:\n",
    "    reader = csv.reader(f)\n",
    "    writer = csv.writer(f_out)\n",
    "\n",
    "    header = next(reader)  # This gets the header row\n",
    "    writer.writerow(header)  # Write the header to the new file\n",
    "\n",
    "    # Find the index of the date column (replace 'date_column' with your column's name)\n",
    "    date_index = header.index('round_type')\n",
    "\n",
    "    series_b_or_above = ['series_b', 'series_c', 'series_d', 'series_e', 'series_f', 'series_g']\n",
    "\n",
    "    # Initialize a counter for the number of rows written\n",
    "    rows_written = 0\n",
    "\n",
    "    # Read the rest of the csv and write rows where the date is within the range\n",
    "    for row in reader:\n",
    "        series_str = row[date_index]\n",
    "        if series_str:  # Check if the date string is not empty\n",
    "            if series_str not in series_b_or_above:\n",
    "                writer.writerow(row)\n",
    "                rows_written += 1\n",
    "                if rows_written % 100 == 0:  # Check if 100 rows have been written\n",
    "                    print(f\"{rows_written} rows written\")\n",
    "                if rows_written == 100:  # Stop after 100 rows\n",
    "                    break\n",
    "        else:\n",
    "            print(\"Warning: Empty date string encountered in row {}\".format(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5eaed7",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737c5f49",
   "metadata": {},
   "source": [
    "Filter Out Series B or Above\n",
    "After merging, filter out organizations that have raised a Series B or above. Assuming the funding round information is stored in a column named round_type in the funding_rounds DataFrame, you can do this by excluding rows where round_type is in a list of Series B or above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
